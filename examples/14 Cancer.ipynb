{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 Cancer Microarray\n",
    "**PAGE 654.** Ramaswamy et al. (2001) present a difficult microarray classification problem, involving a training set of 144 patients with 14 different types of cancer, and a test set of 54 patients. Gene expression measurements were available for 16,063 genes.\n",
    "\n",
    "**DATA INFO.** One gene per row, one sample per column. Cancer classes are labelled as follows:\n",
    "\n",
    "|     |           |     |        |     |        |\n",
    "|----:|:----------|----:|:-------|----:|:-------|\n",
    "|1.   |breast     |2.   |prostate|3.   |lung    |\n",
    "|4.   |collerectal|5.   |lymphoma|6.   |bladder |\n",
    "|7.   |melanoma   |8.   |uterus  |9.   |leukemia|\n",
    "|10.  |renal      |11.  |pancreas|12.  |ovary   |\n",
    "|13.  |meso       |14.  |cns     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, Normalizer\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# define commonly used colors\n",
    "GRAY1, GRAY4, PURPLE = '#231F20', '#646369', '#A020F0'\n",
    "BLUE, ORANGE, BLUE1 = '#57B5E8', '#E69E00', '#174A7E'\n",
    "# configure plot font family to Arial\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['axes.linewidth'] = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../data/14cancer.npy.npz')['data']\n",
    "# last column contains 'is test' flag\n",
    "is_test = data[:, -1].astype(int)\n",
    "data_test = data[is_test == 1, :]\n",
    "data_train = data[is_test == 0, :]\n",
    "# pre-last column contains class\n",
    "y_train = data_train[:, -2].astype(int) - 1\n",
    "y_test = data_test[:, -2].astype(int) - 1\n",
    "X_train = data_train[:, :-2]\n",
    "X_test = data_test[:, :-2]\n",
    "\n",
    "# all 144 training samples are slitted into 8 CV folds\n",
    "cv_indices = np.array([\n",
    "    [5,   2,   1,   3,   6,   4,   7,   8],\n",
    "    [14,  15,  12,  9,   11,  16,  10,  13],\n",
    "    [23,  19,  20,  17,  21,  24,  18,  22],\n",
    "    [31,  32,  29,  28,  26,  30,  25,  27],\n",
    "    [35,  48,  38,  46,  42,  34,  47,  33],\n",
    "    [44,  45,  41,  40,  37,  43,  39,  36],\n",
    "    [55,  56,  49,  51,  53,  50,  52,  54],\n",
    "    [63,  59,  64,  61,  60,  62,  57,  58],\n",
    "    [69,  71,  67,  66,  72,  68,  70,  65],\n",
    "    [87,  91,  76,  86,  81,  88,  83,  96],\n",
    "    [92,  74,  89,  93,  95,  84,  79,  73],\n",
    "    [85,  90,  75,  77,  82,  94,  80,  78],\n",
    "    [99,  103, 98,  100, 97,  104, 102, 101],\n",
    "    [105, 111, 106, 109, 107, 112, 108, 110],\n",
    "    [117, 118, 120, 113, 116, 115, 119, 114],\n",
    "    [128, 121, 122, 124, 125, 127, 123, 126],\n",
    "    [133, 139, 137, 138, 132, 142, 144, 135],\n",
    "    [136, 129, 130, 134, 141, 131, 143, 140]])\n",
    "\n",
    "# in order to be used in GridSearchCV we need to reformat\n",
    "# cv folds into the list of train-test indices\n",
    "cv_indices = (cv_indices.T - 1).tolist()\n",
    "cv_folds = []\n",
    "for i in range(len(cv_indices)):\n",
    "    train = [j for i in cv_indices[:i] + cv_indices[i + 1:] for j in i]\n",
    "    cv_folds.append([train, cv_indices[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "Let's write an auxilar function that calculates CV errors out of 144, its standard error and test errors out of 54. And one additional function for printing these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cv_stat(grid_search):\n",
    "    cv_errors = 18*(1 - np.vstack(\n",
    "        [grid_search.cv_results_[f'split{i}_test_score']\n",
    "         for i in range(8)]).T)\n",
    "    best_cv_errors = cv_errors[grid_search.best_index_, :]\n",
    "    cv_errors_cnt = np.sum(best_cv_errors)\n",
    "    cv_errors_cnt_std = np.sqrt(np.var(best_cv_errors, ddof=1)*8)\n",
    "    test_errors_cnt = np.sum(\n",
    "        grid_search.best_estimator_.predict(X_test) != y_test)\n",
    "    return cv_errors_cnt, cv_errors_cnt_std, test_errors_cnt\n",
    "\n",
    "\n",
    "def print_cv_stat(grid_search):\n",
    "    cv_err, cv_err_std, test_err = calc_cv_stat(grid_search)\n",
    "    print(grid_search.best_params_)\n",
    "    print(f'   CV errors {cv_err} ({cv_err_std:.1f}), Test errors {test_err}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Shrunken Centroids\n",
    "Let's implement nearest shrunken centroid model ourselves. The implementation is the same as proposed in the book, but the result for this task is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShrunkenCentroid(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Nearest shrunken centroid classifier.\n",
    "    Each class is represented by its centroid, with test samples classified to\n",
    "    the class with the nearest centroid.\n",
    "    Parameters\n",
    "    ----------\n",
    "    delta : float, optional (default = None)\n",
    "        Delta for shrinking centroids to remove features.\n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_:\n",
    "        All classes found in the training data set.\n",
    "    centroids_ :\n",
    "        Centroid of each class.\n",
    "    overall_centroid_:\n",
    "        The overall mean of each feature.\n",
    "    priors_:\n",
    "        Class prior probabilities.\n",
    "    vars_:\n",
    "        Pooled within-class variances of features.\n",
    "    shrunken_centroids_:\n",
    "        Shrunken centroid of each class.\n",
    "    features_used_:\n",
    "        The indices of features that are not shrunken to the overall centroid.\n",
    "    \"\"\"\n",
    "    def __init__(self, delta: float = 0):\n",
    "        self.delta = delta\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.array) -> 'ShrunkenCentroid':\n",
    "        \"\"\"\n",
    "        Fit the ShrunkenCentroid model according to the given training data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X :\n",
    "            Training vector, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y :\n",
    "            Target values (integers)\n",
    "        \"\"\"\n",
    "        # relabel target values to start from zero\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(y)\n",
    "        self.classes_ = label_encoder.classes_\n",
    "        N, p, K = *X.shape, self.classes_.size\n",
    "        # calculated overall centroid and prior probabilities\n",
    "        self.overall_centroid_ = np.mean(X, axis=0)\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        self.priors_ = counts / N\n",
    "        # calculate correction coefficients for each class\n",
    "        m = np.atleast_2d((1/counts - 1/N)**0.5).T\n",
    "        self.centroids_ =\\\n",
    "            np.vstack([np.mean(X[y == k, :], axis=0) for k in range(K)])\n",
    "        # pooled within-class variance and deviation of features\n",
    "        self.vars_ = np.zeros(shape=p)\n",
    "        for k in range(K):\n",
    "            self.vars_ += np.sum((X[y == k, :]-self.centroids_[k])**2, axis=0)\n",
    "        self.vars_ /= (N - K)\n",
    "        stds = self.vars_ ** 0.5\n",
    "        # calculate shrunken centroids\n",
    "        distances = self.centroids_ - self.overall_centroid_\n",
    "        mean_std = np.median(stds)\n",
    "        t_stats = distances / (stds + mean_std) / m\n",
    "        t_stats_shrunken =\\\n",
    "            np.sign(t_stats) * (np.abs(t_stats) - self.delta).clip(0)\n",
    "        self.shrunken_centroids_ =\\\n",
    "            self.overall_centroid_ + m * (stds + mean_std) * t_stats_shrunken\n",
    "        self.features_used_ = np.squeeze(np.argwhere(np.sum(np.abs(\n",
    "            self.shrunken_centroids_ - self.overall_centroid_), axis=0) > 0))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Perform classification on an array of test vectors X.\n",
    "        The predicted class C for each sample in X is returned.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape = [n_samples]\n",
    "        \"\"\"\n",
    "        N, K = X.shape[0], self.classes_.size\n",
    "        discriminators = np.zeros(shape=(N, K))\n",
    "        for i in range(N):\n",
    "            discriminators[i] = -np.sum(\n",
    "                (self.shrunken_centroids_ - X[i])**2 / self.vars_, axis=1) +\\\n",
    "                                2*np.log(self.priors_)\n",
    "        return self.classes_[np.argmax(discriminators, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__delta': 2.7777777777777777}\n",
      "   CV errors 34.0 (4.7), Test errors 17\n",
      "   Number of Genes Used 5417\n"
     ]
    }
   ],
   "source": [
    "shrunken_centroid_clf = Pipeline([\n",
    "    ('norm', Normalizer()),\n",
    "    ('classifier', ShrunkenCentroid())])\n",
    "\n",
    "shrunken_centroid_gs = GridSearchCV(\n",
    "    shrunken_centroid_clf,\n",
    "    {'classifier__delta': np.linspace(0, 5, 10)},\n",
    "    cv=cv_folds, iid=True, scoring='accuracy'\n",
    ").fit(X_train, y_train)\n",
    "sc_n_genes = shrunken_centroid_gs.best_estimator_[1].features_used_.size\n",
    "print_cv_stat(shrunken_centroid_gs)\n",
    "print(f'   Number of Genes Used {sc_n_genes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Penalized Discriminant Analysis\n",
    "**PAGE 659.** The computational techniques discussed in this section apply to any method that ftis a linear model with quadratic regularization on the coefficients. That includes all the methods discussed in this section, and many more. When $p > N$, the computations can be carried out in an $N$-dimensional space, rather than $p$, via the singular value decomposition introduced in Section 14.5. Here is the geometric intuition: just like two points in three-dimensional space always lie on a line, $N$ points in $p$-dimensional space lie in an $(N-1)$-dimensional affine subspace.\n",
    "\n",
    "Given then $N \\times p$ data matrix $\\mathbf{X}$, let $$\\mathbf{X} = \\mathbf{U}\\mathbf{D}\\mathbf{V}^T = \\mathbf{R}\\mathbf{V}^T$$\n",
    "be the singular-value decomposition (SVD) of $\\mathbf{X}$; that is, $\\mathbf{V}$ is $p \\times N$ with orthonormal columns, $\\mathbf{U}$ is $N \\times N$ orthogonal, and $\\mathbf{D}$ a diagonal matrix with elements $d_1 \\ge d_2 \\ge d_N \\ge 0$. The matrix $\\mathbf{R}$ is $N \\times N$, with rows $r_i^T$.\n",
    "\n",
    "As a simple example, let's first consider the estimates from a ridge regression:\n",
    "$$\\hat{\\beta} = (\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "Replacing $\\mathbf{X}$ by $\\mathbf{R}\\mathbf{V}^T$ and after some further manipulations, this can be shown to equal\n",
    "$$\\hat{\\beta} = \\mathbf{V}(\\mathbf{R}^T\\mathbf{R} + \\lambda \\mathbf{I})^{-1}\\mathbf{R}^T\\mathbf{y}$$\n",
    "(Exercise 18.4). Thus $\\hat{\\beta}=\\mathbf{V}\\hat{\\theta}$, where $\\hat{\\theta}$ is the ridge-regression estimate using the $N$ observations $(r_i, y_i), i=1, 2, ..., N$. In other wrods, we can simply reduce the data matrix from $\\mathbf{X}$ to $\\mathbf{R}$, and work with the rows of $\\mathbf{R}$. This trick reduces the computational costs from $O(p^3)$ to $O(pN^2)$ when $p > N$.\n",
    "\n",
    "**PAGE 660.** Let $f^*(r_i) = \\theta_0 + r_i^T\\theta$ with $r_i$ defined in (18.13), and consider the pair of optimization problems:\n",
    "$$(\\hat{\\beta}_0, \\hat{\\beta}) = \\arg \\min_{\\beta_0, \\beta \\in \\mathbb{R}^p}\\sum_{i=1}^{N}L(y_i, \\beta_0+x_i^T\\beta) + \\lambda \\beta^T\\beta$$\n",
    "$$(\\hat{\\theta}_0, \\hat{\\theta}) = \\arg \\min_{\\theta_0, \\theta \\in \\mathbb{R}^N}\\sum_{i=1}^{N}L(y_i, \\theta_0+r_i^T\\theta) + \\lambda \\theta^T\\theta$$\n",
    "Then the $\\hat{\\beta}_0 = \\hat{\\theta}_0$, and $\\hat{\\beta}=\\mathbf{V}\\hat{\\theta}$.\n",
    "\n",
    "The theorem says that we can simply replace the $p$ vectors $x_i$ by the $N$-vectors $r_i$, and perform our penalized fit as before, but with far fewer predictors. The $N$-vector solution $\\hat{\\theta}$ is then transformed back to the $p$-vector solution via a simple matrix multiplication. This result is part of the statistics folklore, and deserve to be known more widely - see Hastie and Tibshirani (2004) for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2PenalizedDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"L2 Penalized Discriminant Analysis\n",
    "    A classifier for high-dimensional problems when p>>N. Uses transformed\n",
    "    space to optimize calculations.\n",
    "    Parameters\n",
    "    ----------\n",
    "    lam :\n",
    "        How much the variance-covariance matrix should be shrunken towards\n",
    "        its diagonal. Lower value indicates higher shrinkage and more penalty.\n",
    "    \"\"\"\n",
    "    def __init__(self, lam=0.99):\n",
    "        self.lam = lam\n",
    "\n",
    "    def fit(self,\n",
    "            X: np.ndarray,\n",
    "            y: np.array) -> 'L2PenalizedDiscriminantAnalysis':\n",
    "        \"\"\"Fit L2PenalizedDiscriminantAnalysis model according to the given\n",
    "           training data and parameters.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X :\n",
    "            Training data.\n",
    "        y :\n",
    "            Target values.\n",
    "        \"\"\"\n",
    "        # relabel target values to start from zero\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(y)\n",
    "        self.classes_ = label_encoder.classes_\n",
    "        K, N, p = self.classes_.size, X.shape[0], X.shape[1]\n",
    "        # calculate classes priors\n",
    "        _, counts_elements = np.unique(y, return_counts=True)\n",
    "        priors = counts_elements/y.shape[0]\n",
    "        # transform the predictors space using SVD\n",
    "        u, s, vh = np.linalg.svd(X, full_matrices=False)\n",
    "        U, D, V = u, np.diag(s), vh.T\n",
    "        R = U @ D\n",
    "        # calculate means and the variance-covariance matrix\n",
    "        means = [np.mean(R[y == i], axis=0) for i in range(K)]\n",
    "        covs = np.zeros((N, N))\n",
    "        for k in range(K):\n",
    "            R_k = R[y == k, :]\n",
    "            for i in range(R_k.shape[0]):\n",
    "                xc = np.atleast_2d(R_k[i, :] - means[k]).T\n",
    "                covs += (xc @ xc.T) / (N - K)\n",
    "        covs = self.lam * covs + (1 - self.lam) * np.diag(np.diag(covs))\n",
    "        covs_inv = np.linalg.inv(covs)\n",
    "        # calculate intercepts and coefficients in the original space\n",
    "        self.coef_, self.intercept_ = [], []\n",
    "        for k in range(K):\n",
    "            m_k = np.atleast_2d(means[k]).T\n",
    "            p_k = priors[k]\n",
    "            self.coef_.append(V @ covs_inv @ m_k)\n",
    "            self.intercept_.append(-0.5 * m_k.T @ covs_inv @ m_k + np.log(p_k))\n",
    "        self.intercept_ = np.hstack(self.intercept_)\n",
    "        self.coef_ = np.hstack(self.coef_)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.array:\n",
    "        scores = X@self.coef_ + self.intercept_\n",
    "        return self.classes_[np.argmax(scores, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__lam': 0.8}\n",
      "   CV errors 20.0 (3.0), Test errors 11\n"
     ]
    }
   ],
   "source": [
    "penalized_discriminant_clf = Pipeline([\n",
    "    ('norm', Normalizer()),\n",
    "    ('classifier', L2PenalizedDiscriminantAnalysis())])\n",
    "\n",
    "penalized_discriminant_gs = GridSearchCV(\n",
    "    penalized_discriminant_clf,\n",
    "    {'classifier__lam': np.linspace(0.1, 0.9, 9)},\n",
    "    cv=cv_folds, iid=True, scoring='accuracy'\n",
    ").fit(X_train, y_train)\n",
    "print_cv_stat(penalized_discriminant_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__C': 1000.0}\n",
      "   CV errors 27.0 (2.6), Test errors 14\n"
     ]
    }
   ],
   "source": [
    "support_vector_clf = Pipeline([\n",
    "    ('norm', Normalizer()),\n",
    "    ('classifier', LinearSVC(tol=1e-3))])\n",
    "\n",
    "support_vector_gs = GridSearchCV(\n",
    "    support_vector_clf,\n",
    "    {'classifier__C': np.linspace(1000, 3000, 3)},\n",
    "    cv=cv_folds, iid=True, scoring='accuracy'\n",
    ").fit(X_train, y_train)\n",
    "print_cv_stat(support_vector_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__n_neighbors': 1}\n",
      "   CV errors 47.0 (3.8), Test errors 24\n"
     ]
    }
   ],
   "source": [
    "k_nearest_neighbors_clf = Pipeline([\n",
    "    ('norm', Normalizer()),\n",
    "    ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "k_nearest_neighbors_gs = GridSearchCV(\n",
    "    k_nearest_neighbors_clf,\n",
    "    {'classifier__n_neighbors': list(range(1, 5))},\n",
    "    cv=cv_folds, iid=True, scoring='accuracy'\n",
    ").fit(X_train, y_train)\n",
    "print_cv_stat(k_nearest_neighbors_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Penalized Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__alpha': 0.016666666666666666}\n",
      "   CV errors 24.0 (3.7), Test errors 16\n"
     ]
    }
   ],
   "source": [
    "l2sgd = SGDClassifier(\n",
    "    loss='log', penalty='l2', alpha=0.05, max_iter=10000, n_jobs=4,\n",
    "    tol=1e-5, eta0=0.0005, learning_rate='adaptive', random_state=1)\n",
    "l2_multinom_clf = Pipeline([\n",
    "    ('norm', Normalizer()),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('classifier', l2sgd)])\n",
    "l2_multinom_gs = GridSearchCV(\n",
    "    l2_multinom_clf,\n",
    "    {'classifier__alpha': np.linspace(0.01, 0.03, 4)},\n",
    "    cv=cv_folds, iid=True, scoring='accuracy'\n",
    ").fit(X_train, y_train)\n",
    "print_cv_stat(l2_multinom_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Penalized Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__alpha': 0.05}\n",
      "   CV errors 23.000000000000004 (4.4), Test errors 13\n",
      "   Number of Genes Used 278\n"
     ]
    }
   ],
   "source": [
    "l1sgd = SGDClassifier(\n",
    "    loss='log', penalty='l1', alpha=0.05, max_iter=10000, n_jobs=4,\n",
    "    tol=1e-5, eta0=0.0005, learning_rate='adaptive', random_state=1)\n",
    "l1_multinom_clf = Pipeline([\n",
    "    ('norm', Normalizer()),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('classifier', l1sgd)])\n",
    "\n",
    "# uncomment the line to run full grid search, it works realy slow\n",
    "l1_multinom_gs = GridSearchCV(\n",
    "    l1_multinom_clf,\n",
    "    # {'classifier__alpha': np.linspace(0.03, 0.09, 7)},\n",
    "    {'classifier__alpha': [0.05]},\n",
    "    cv=cv_folds, iid=True, scoring='accuracy'\n",
    ").fit(X_train, y_train)\n",
    "l1_multinom_n_genes = np.sum(np.sum(\n",
    "    abs(l1_multinom_gs.best_estimator_[2].coef_), axis=0) != 0)\n",
    "print_cv_stat(l1_multinom_gs)\n",
    "print(f'   Number of Genes Used {l1_multinom_n_genes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic-net Penalized Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__alpha': 0.08, 'classifier__l1_ratio': 0.6}\n",
      "   CV errors 25.0 (5.5), Test errors 12\n",
      "   Number of Genes Used 358\n"
     ]
    }
   ],
   "source": [
    "elastic_net_sgd = SGDClassifier(\n",
    "    loss='log', penalty='elasticnet', alpha=0.08, l1_ratio=0.6, max_iter=10000,\n",
    "    n_jobs=4, tol=1e-5, eta0=0.0005, learning_rate='adaptive', random_state=1)\n",
    "elastic_net_clf = Pipeline([\n",
    "    ('norm', Normalizer()),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('classifier', elastic_net_sgd)])\n",
    "\n",
    "# uncomment the line to run full grid search, it works realy slow\n",
    "elastic_net_gs = GridSearchCV(\n",
    "    elastic_net_clf,\n",
    "    {'classifier__alpha': [0.08],  # [0.07, 0.08, 0.09],\n",
    "     'classifier__l1_ratio': [0.6]},  # [0.5, 0.6, 0.7]},\n",
    "    cv=cv_folds, iid=True, scoring='accuracy'\n",
    ").fit(X_train, y_train)\n",
    "elastic_net_n_genes = np.sum(np.sum(\n",
    "    abs(elastic_net_gs.best_estimator_[2].coef_), axis=0) != 0)\n",
    "print_cv_stat(elastic_net_gs)\n",
    "print(f'   Number of Genes Used {elastic_net_n_genes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['1. Nearest shrunken centroids',\n",
    "         '2. L2-penalized discriminant|   analysis',\n",
    "         '3. Support vector classifier',\n",
    "         '5. k-nearest neighbors',\n",
    "         '6. L2-penalized multinominal',\n",
    "         '7. L1-penalized multinominal',\n",
    "         '8. Elastic-net penalized|   multinominal']\n",
    "grid_searches = [\n",
    "    shrunken_centroid_gs, penalized_discriminant_gs, support_vector_gs,\n",
    "    k_nearest_neighbors_gs, l2_multinom_gs, l1_multinom_gs, elastic_net_gs]\n",
    "genes_used_cnt = [sc_n_genes, 16063, 16063, 16063, 16063, l1_multinom_n_genes,\n",
    "                  elastic_net_n_genes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methods                        CV errors (SE)  Test errors   Number of\n",
      "                               Out of 144      out of 54    Genes Used\n",
      "----------------------------------------------------------------------\n",
      "1. Nearest shrunken centroids  34 (4.7)        17                5,417\n",
      "2. L2-penalized discriminant   20 (3.0)        11               16,063\n",
      "   analysis\n",
      "3. Support vector classifier   27 (2.6)        14               16,063\n",
      "5. k-nearest neighbors         47 (3.8)        24               16,063\n",
      "6. L2-penalized multinominal   24 (3.7)        16               16,063\n",
      "7. L1-penalized multinominal   23 (4.4)        13                  278\n",
      "8. Elastic-net penalized       25 (5.5)        12                  358\n",
      "   multinominal\n"
     ]
    }
   ],
   "source": [
    "# PAGE 656. TABLE 18.1. Prediction results for microarray data with 14 cancer\n",
    "#           classes. Method 1 is described in Section 18.2. Method 2, 3 and 6\n",
    "#           are discussed in Section 18.3, while 4, 7 and 8 are discussed in\n",
    "#           Section 18.4. Method 5 is described in Section 13.3. The elastic-\n",
    "#           net penalized multinomial does the best on the test data, but the\n",
    "#           standard error of each test-error estimate is about 3, so such\n",
    "#           comparisons are inconclusive.\n",
    "print('Methods                        CV errors (SE)  Test errors   Number of')\n",
    "print('                               Out of 144      out of 54    Genes Used')\n",
    "print('----------------------------------------------------------------------')\n",
    "for name, gs, genes_cnt in zip(names, grid_searches, genes_used_cnt):\n",
    "    name = name.split('|')\n",
    "    cv_err, cv_err_std, test_err = calc_cv_stat(gs)\n",
    "    print(f'{name[0]:<29}{cv_err:>4.0f} ({cv_err_std:.1f}){test_err:>10}'\n",
    "          f'{genes_cnt:>21,}')\n",
    "    if len(name) > 1:\n",
    "        print(name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (with Tensorflow GPU)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
